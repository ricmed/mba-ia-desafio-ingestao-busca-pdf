# Desafio MBA Engenharia de Software com IA - Full Cycle

## IngestÃ£o e Busca SemÃ¢ntica com LangChain e PostgreSQL

Sistema completo de ingestÃ£o e busca semÃ¢ntica de documentos PDF usando LangChain, PostgreSQL com extensÃ£o pgVector, e modelos de IA (OpenAI ou Google Gemini).

## ğŸ“‹ PrÃ©-requisitos

- Python 3.12+
- Docker e Docker Compose
- API Key da OpenAI ou Google Gemini
- Ambiente virtual Python (jÃ¡ criado como `.venv`)

## ğŸš€ InstalaÃ§Ã£o

1. **Clone o repositÃ³rio** (se ainda nÃ£o tiver feito):
```bash
git clone <url-do-repositorio>
cd mba-ia-desafio-ingestao-busca-pdf
```

2. **Ative o ambiente virtual**:
```bash
source .venv/bin/activate
```

3. **Configure as variÃ¡veis de ambiente**:
   - Copie o arquivo `.env.example` para `.env` (se existir) ou crie um arquivo `.env` na raiz do projeto
   - Adicione suas chaves de API:
```env
# OpenAI Configuration
OPENAI_API_KEY=sua_chave_openai_aqui

# Google Gemini Configuration (opcional)
GOOGLE_API_KEY=sua_chave_google_aqui

# Provider Selection: "openai" ou "gemini"
EMBEDDING_PROVIDER=openai
LLM_PROVIDER=openai

# PDF Path (padrÃ£o: document.pdf na raiz do projeto)
PDF_PATH=document.pdf

# Database Configuration (padrÃµes jÃ¡ configurados)
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=rag
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
```

4. **Coloque o arquivo PDF** na raiz do projeto com o nome `document.pdf` (ou ajuste a variÃ¡vel `PDF_PATH` no `.env`)

## ğŸ³ Executando o Banco de Dados

Suba o PostgreSQL com pgVector usando Docker Compose:

```bash
docker compose up -d
```

Aguarde alguns segundos para o banco estar pronto. O serviÃ§o `bootstrap_vector_ext` criarÃ¡ automaticamente a extensÃ£o `vector` no PostgreSQL.

Para verificar se estÃ¡ rodando:
```bash
docker compose ps
```

## ğŸ“¥ IngestÃ£o do PDF

Execute o script de ingestÃ£o para processar o PDF e armazenar os embeddings no banco de dados:

```bash
python src/ingest.py
```

O script irÃ¡:
- Carregar o PDF
- Dividir em chunks de 1000 caracteres com overlap de 150
- Gerar embeddings usando o modelo configurado
- Armazenar os vetores no PostgreSQL

**Nota**: Se vocÃª jÃ¡ executou a ingestÃ£o anteriormente e quer reprocessar, pode ser necessÃ¡rio limpar a tabela existente ou usar um nome de coleÃ§Ã£o diferente.

## ğŸ’¬ Chat Interativo

Execute o chat para fazer perguntas sobre o conteÃºdo do PDF:

```bash
python src/chat.py
```

Exemplo de uso:
```
PERGUNTA: Qual o faturamento da Empresa SuperTechIABrazil?
RESPOSTA: O faturamento foi de 10 milhÃµes de reais.

PERGUNTA: Quantos clientes temos em 2024?
RESPOSTA: NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta.
```

Para sair do chat, digite `sair`, `exit`, `quit` ou `q`.

## ğŸ”§ ConfiguraÃ§Ã£o de Modelos

### OpenAI
- **Embeddings**: `text-embedding-3-small`
- **LLM**: `gpt-4o-mini` (o modelo `gpt-5-nano` mencionado nÃ£o existe, usando `gpt-4o-mini` como alternativa)

### Google Gemini
- **Embeddings**: `models/embedding-001`
- **LLM**: `gemini-2.5-flash-lite`

Para alternar entre providers, ajuste as variÃ¡veis `EMBEDDING_PROVIDER` e `LLM_PROVIDER` no arquivo `.env`.

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ docker-compose.yml          # ConfiguraÃ§Ã£o do PostgreSQL com pgVector
â”œâ”€â”€ requirements.txt            # DependÃªncias Python
â”œâ”€â”€ .env                        # VariÃ¡veis de ambiente (nÃ£o versionado)
â”œâ”€â”€ document.pdf                # PDF para ingestÃ£o
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py              # Script de ingestÃ£o do PDF
â”‚   â”œâ”€â”€ search.py              # Busca semÃ¢ntica e geraÃ§Ã£o de resposta
â”‚   â””â”€â”€ chat.py                # CLI interativo para perguntas
â””â”€â”€ README.md                   # Este arquivo
```

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3.12+**
- **LangChain**: Framework para aplicaÃ§Ãµes com LLMs
- **PostgreSQL + pgVector**: Banco de dados vetorial
- **Docker & Docker Compose**: ContainerizaÃ§Ã£o do banco de dados
- **OpenAI API**: Modelos de embeddings e LLM
- **Google Gemini API**: Alternativa para embeddings e LLM

## ğŸ“ Notas Importantes

1. **Primeira execuÃ§Ã£o**: Certifique-se de executar `docker compose up -d` antes de rodar a ingestÃ£o
2. **ReingestÃ£o**: Se precisar reprocessar o PDF, vocÃª pode precisar limpar os dados anteriores ou ajustar o cÃ³digo
3. **Custos**: O uso de APIs da OpenAI e Google Gemini pode gerar custos. Monitore seu uso
4. **Modelo LLM**: O modelo `gpt-5-nano` mencionado no requisito nÃ£o existe. O cÃ³digo usa `gpt-4o-mini` como alternativa compatÃ­vel

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro de conexÃ£o com o banco
- Verifique se o Docker estÃ¡ rodando: `docker compose ps`
- Verifique se o PostgreSQL estÃ¡ saudÃ¡vel: `docker compose logs postgres`

### Erro de API Key
- Verifique se o arquivo `.env` existe e contÃ©m as chaves corretas
- Certifique-se de que as chaves nÃ£o tÃªm espaÃ§os extras

### Erro ao importar mÃ³dulos
- Ative o ambiente virtual: `source .venv/bin/activate`
- Verifique se todas as dependÃªncias estÃ£o instaladas: `pip list`

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido como parte do desafio do MBA em Engenharia de Software com IA.